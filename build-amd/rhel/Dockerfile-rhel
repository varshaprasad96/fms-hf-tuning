ARG BASE_IMAGE="registry.access.redhat.com/ubi9/ubi:latest"

################################ Build ROCm base image ###############################
FROM ${BASE_IMAGE} as rocm-base

RUN echo "Base image is ${BASE_IMAGE}"

RUN <<EOF
cat <<EOD > /etc/yum.repos.d/amdgpu.repo
[amdgpu]
name=amdgpu
baseurl=https://repo.radeon.com/amdgpu/6.1.2/rhel/9.4/main/x86_64/
enabled=1
priority=50
gpgcheck=1
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key

[ROCm-6.1.2]
name=ROCm6.1.2
baseurl=https://repo.radeon.com/rocm/rhel9/6.1.2/main
enabled=1
priority=50
gpgcheck=1
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key
EOD
EOF

RUN yum -y install rocm hipcc && yum clean all

# Add ROCm libraries to the linker path
RUN  tee --append /etc/ld.so.conf.d/rocm.conf <<EOF
/opt/rocm/lib
/opt/rocm/lib64
EOF

# Set environment variables for ROCm
ENV PATH=$PATH:/opt/rocm-6.1.2/bin

################################################### Build pytorch #####################
FROM rocm-base as pytorch-build

ARG FA_GFX_ARCHS="gfx90a;gfx942"
RUN echo "FA_GFX_ARCHS is $FA_GFX_ARCHS"

ARG FA_BRANCH="ae7928c"
RUN echo "FA_BRANCH is $FA_BRANCH"

# Install basic python utilities
# TODO: Patch is required by flash-attention, to be able to fix hipify - verify if its still required.
RUN yum -y install python3 python3-pip python3-devel patch

# Install some basic utilities
RUN yum -y install git bzip2 libX11-devel wget unzip https://mirror.stream.centos.org/9-stream/BaseOS/x86_64/os/Packages/tmux-3.2a-4.el9.x86_64.rpm && yum clean all


RUN python3 -m pip install --upgrade pip
RUN python3 -m pip install --no-cache-dir fastapi ninja tokenizers pandas
RUN python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0

ENV PATH=$PATH:/opt/rocm/bin:/libtorch/bin:
ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/rocm/lib/:/libtorch/lib:
ENV CPLUS_INCLUDE_PATH=$CPLUS_INCLUDE_PATH:/libtorch/include:/libtorch/include/torch/csrc/api/include/:/opt/rocm/include/

################################################### Build flash-attention #####################
FROM pytorch-build as flashattention-build

# Install ROCm flash-attention
# Clone and install Flash Attention with ROCm support
# Refer https://rocm.blogs.amd.com/artificial-intelligence/flash-attention/README.html#getting-started
# TODO:
# For setup.py from flash-attention to be installed successfully it needs to be able to recognize
# amd GPUs.
# A local run indicates from https://github.com/ROCm/flash-attention/blob/flash_attention_for_rocm/setup.py indicates
# cuda env vars need to be set. Verify if that's still the case, after AMD_GPU env is recognized.
# Refer: https://github.com/ROCm/flash-attention/blob/flash_attention_for_rocm/Dockerfile.rocm
RUN mkdir libs \
    && cd libs \
    && git clone https://github.com/ROCm/flash-attention.git \
    && cd flash-attention \
    && git submodule update --init \
    && python3 setup.py install --verbose \
    && cd .. # Back to parent dir

# build triton
RUN mkdir -p libs \
    && cd libs \
    && pip uninstall -y triton \
    && git clone https://github.com/ROCm/triton.git \
    && cd triton/python \
    && pip3 install . \
    && cd ../..

################################## Build fms-tuning image ###########################



